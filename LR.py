# -*- coding: utf-8 -*-
"""LR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Py4cKbJuMpgQqJKIwreRYZ22V192c56O
"""

import pandas as pd
import numpy as np
#import matplotlib
#matplotlib.use("Agg")
import matplotlib.pyplot as plt

"""#Scalar"""

class Scaler():
    # hint: https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/
    def __init__(self):
        pass
        # raise NotImplementedError
    def __call__(self,features, is_train=False):
        self.features = features
        pass
    def numerical_scaling(self,variable):
        for i in variable:
            #min-max scaler
            self.feature_update[i] = (self.feature_update[i]-min(self.feature_update[i]))/(max(self.feature_update[i])-min(self.feature_update[i]))
            mean = np.mean(self.feature_update[i])
            std = np.std(self.feature_update[i])
            self.feature_update[i] = (self.feature_update[i]-mean)/std #Standard scaler
        pass
    def categorical_scaling(self,variable):
        for i in variable:
            self.feature_update[i] = self.feature_update[i].astype('category')
            self.feature_update[i] = self.feature_update[i].cat.codes
            pass
        pass
    def lat_lon_scaling(self):
        pass
    def feature_vary(self):
        self.feature_update['month'] = pd.DatetimeIndex(self.features['acq_date']).month
        self.feature_update['year'] = pd.DatetimeIndex(self.features['acq_date']).year
        self.feature_update['day'] = pd.DatetimeIndex(self.features['acq_date']).day
        self.feature_update = self.feature_update.drop(['acq_date'],axis=1)
    def perform_scaling(self):
        self.feature_update = self.features.copy()
        self.feature_vary()
        categorical_variables = ['satellite','daynight','month','year','day']
        numerical_variables = ['brightness','acq_time','bright_t31','latitude','longitude','scan','track','satellite','daynight','month','year','day']
        self.categorical_scaling(categorical_variables)
        self.numerical_scaling(numerical_variables)
        # date_time_sclaing()
        return self.feature_update,self.features
        # raise NotImplementedError

"""#Get Features"""

def get_features(csv_path,is_train=False,scaler=None):
    '''
    Description:
    read input feature columns from csv file
    manipulate feature columns, create basis functions, do feature scaling etc.
    return a feature matrix (numpy array) of shape m x n 
    m is number of examples, n is number of features
    return value: numpy array''' '''  ''' '''
    '''
    feats_csv=pd.read_csv(csv_path,index_col=0)
    feats_csv = feats_csv.drop(['instrument','version'],axis=1)
    if (csv_path != 'data/test.csv'):
        feats_csv = feats_csv.drop(['frp'],axis=1)
    scale = Scaler()
    scale(feats_csv)
    feature_scaled,feature_unscaled = scale.perform_scaling()
    
    # feature_scaled = feature_unscaled[['brightness','scan']]
    # feature_scaled = feature_scaled.drop(['satellite','daynight','month','year'],axis=1)
    feature_mat = np.array(feature_scaled)
    # feature_mat = np.vstack([np.ones(feature_mat.shape[0],),feature_mat]).T
    feature_mat = np.insert(feature_mat, 0, np.ones((feature_mat.shape[0],)), axis = 1)
    '''
    Arguments:
    csv_path: path to csv file
    is_train: True if using training data (optional)
    scaler: a class object for doing feature scaling (optional)
    '''

    '''
    help:
    useful links: 
        * https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html
        * https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/
    '''

    return feature_mat

"""#Get Targets"""

def get_targets(csv_path):
    '''
    Description:
    read target outputs from the csv file
    return a numpy array of shape m x 1
    m is number of examples
    '''
    targets_csv=pd.read_csv(csv_path,index_col=0)
    targets_arr = np.array(targets_csv['frp'])
    
    return targets_arr

"""#Analytical Solution"""

def analytical_solution(feature_matrix, targets, C=0.0):
    x = feature_matrix
    y = targets
    w = x.T@x
    wshape,qshape = w.shape
    w = w +C*np.eye(wshape)
    w = np.linalg.inv(w)@x.T
    return w@y
    raise NotImplementedError

"""#Get Predictions"""

def get_predictions(feature_matrix, weights):
    data = feature_matrix@weights
    return data
    raise NotImplementedError

"""#RMSE Loss"""

def mse_loss(feature_matrix, weights, targets):
    '''
    Description:
    Implement mean squared error loss function
    return value: float (scalar)
    '''

    '''
    Arguments:
    feature_matrix: numpy array of shape m x n
    weights: numpy array of shape n x 1
    targets: numpy array of shape m x 1
    '''
    n = feature_matrix.shape[0]
    predictions = get_predictions(feature_matrix, weights)
    loss = predictions - targets
    return np.sqrt(np.sum(np.square(loss), axis = 0)/n)
    raise NotImplementedError

"""#L2 Regularizer"""

def l2_regularizer(weights):
    '''
    Description:
    Implement l2 regularizer
    return value: float (scalar)
    '''

    '''
    Arguments
    weights: numpy array of shape n x 1
    '''
    weights_sq = np.square(weights)
    return np.sum(weights_sq, axis=0)
    raise NotImplementedError

"""#Loss Function"""

def loss_fn(feature_matrix, weights, targets, C=0.0):
    '''
    Description:
    compute the loss function: mse_loss + C * l2_regularizer
    '''

    '''
    Arguments:
    feature_matrix: numpy array of shape m x n
    weights: numpy array of shape n x 1
    targets: numpy array of shape m x 1
    C: weight for regularization penalty
    return value: float (scalar)
    '''
    return mse_loss(feature_matrix, weights, targets) + C*l2_regularizer(weights)
    raise NotImplementedError

"""#Gradient Computation"""

def compute_gradients(feature_matrix, weights, targets, C=0.0):
  # n = feature_matrix.shape[0]
  # w_update = ((feature_matrix.T)@(targets-(feature_matrix@weights))) + C*weights
  # return 2*w_update/n
    # diff = (targets - get_predictions(feature_matrix, weights))
    # grad = (-2*(feature_matrix.T@diff)/feature_matrix.shape[0]) - 2*C*weights  
    # return grad
    # raise NotImplementedError
  XW = np.matmul(feature_matrix,weights) #calculation of XW
  Y = XW-targets #(XW-Y)
    
    # calculation of XTranspose.(XW-Y)
  XT=feature_matrix.T
  X_Y = np.matmul(XT,Y) 
    
    #calculation of 2*XTranspose.(XW-Y)/now of rows
  a=feature_matrix.shape[0]
  GD_MSE = 2*(X_Y/a) 
    
    #As weight of bias will not be regularized, remove the weight of bias and append 0 for the regularization gradient part 
  GD_Rg=2*C*weights[:-1] 
  GD_RG=np.append(GD_Rg,0.0)
    
  Gradient=(GD_RG+GD_MSE)
    
  return Gradient

"""#Random Batch Size"""

def sample_random_batch(feature_matrix, targets, batch_size):
    '''
    Description
    Batching -- Randomly sample batch_size number of elements from feature_matrix and targets
    return a tuple: (sampled_feature_matrix, sampled_targets)
    sampled_feature_matrix: numpy array of shape batch_size x n
    sampled_targets: numpy array of shape batch_size x 1
    '''

    '''
    Arguments:
    feature_matrix: numpy array of shape m x n
    targets: numpy array of shape m x 1
    batch_size: int
    '''    
    temp = list(range(feature_matrix.shape[0]))
    np.random.shuffle(temp)
    random_sample = temp[:32] 
    sampled_feature_matrix = feature_matrix[random_sample,:]
    sampled_targets = train_targets[random_sample]
    return (sampled_feature_matrix, sampled_targets)
    raise NotImplementedError

"""#Weights Initialization and updation"""

def initialize_weights(n):
    '''
    Description:
    initialize weights to some initial values
    return value: numpy array of shape n x 1
    '''

    '''
    Arguments
    n: int
    '''
    return np.zeros((n,))
    raise NotImplementedError

def update_weights(weights, gradients, lr):
    '''
    Description:
    update weights using gradient descent
    retuen value: numpy matrix of shape nx1
    '''

    '''
    Arguments:
    # weights: numpy matrix of shape nx1
    # gradients: numpy matrix of shape nx1
    # lr: learning rate
    '''    
    return weights-(lr*gradients)
    raise NotImplementedError

def early_stopping(arg_1=None, arg_2=None, arg_3=None, arg_n=None):
    # allowed to modify argument list as per your need
    # return True or False
    return False
    raise NotImplementedError
    

def plot_trainsize_losses(dev_loss_values,train_loss_values,steps):   
    raise NotImplementedError

"""#Gradient Descent"""

def do_gradient_descent(train_feature_matrix,  
                        train_targets, 
                        dev_feature_matrix,
                        dev_targets,
                        lr=1.0,
                        C=0.0,
                        batch_size=32,
                        max_steps=10000,
                        eval_steps=5):
    '''
    feel free to significantly modify the body of this function as per your needs.
    ** However **, you ought to make use of compute_gradients and update_weights function defined above
    return your best possible estimate of LR weights

    a sample code is as follows -- 
    '''
    weights = initialize_weights(train_feature_matrix.shape[1])
    dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)
    train_loss = mse_loss(train_feature_matrix, weights, train_targets)

    print("step {} \t dev loss: {} \t train loss: {}".format(0,dev_loss,train_loss))
    for step in range(1,max_steps+1):

        #sample a batch of features and gradients
        features,targets = sample_random_batch(train_feature_matrix,train_targets,batch_size)
        
        #compute gradients
        gradients = compute_gradients(features, weights, targets, C)
        
        #update weights
        weights = update_weights(weights, gradients, lr)

        if step%eval_steps == 0:
            dev_loss = mse_loss(dev_feature_matrix, weights, dev_targets)
            train_loss = mse_loss(train_feature_matrix, weights, train_targets)
            print("step {} \t dev loss: {} \t train loss: {}".format(step,dev_loss,train_loss))

        '''
        implement early stopping etc. to improve performance.
        '''

    return weights

def do_evaluation(feature_matrix, targets, weights):
    # your predictions will be evaluated based on mean squared error 
    predictions = get_predictions(feature_matrix, weights)
    loss =  loss_fn(feature_matrix, weights, targets,C=0.00001)
    return loss

#basis function
def do_e_raisedto_stuff(matrix2D):
    length = len(matrix2D)
    width = len(matrix2D[0])
    means = []
    variance = []
    e = 2.71828182845905
    for i in range(width):
        means[i] = sum(matrix2D[:,i])/length
        variance[i] = np.std(matrix2D[:,i])
    newmatrix = [[] for _ in range(width)]
    for i in range(width):
        for j in range(length):
            newmatrix[j,i] = e * (-((matrix2D[j,i]-means[i])*2)/(2 * variance[i]))
    return newmatrix

def do_x_squared(matrix2D):
    length = len(matrix2D)
    width = len(matrix2D[0])
    newmatrix = [[] for _ in range(width)]
    for i in range(length):
        for j in range(width):
            newmatrix[i,j] = (matrix2D[i,j]) ** 2
    return newmatrix

def do_x_cubed(matrix2D):
    length = len(matrix2D)
    width = len(matrix2D[0])
    newmatrix = [[] for _ in range(width)]
    for i in range(length):
        for j in range(width):
            newmatrix[i,j] = (matrix2D[i,j]) ** 3
    return newmatrix

def append_basis_functions(matrix2D):
    length = len(matrix2D)
    width = len(matrix2D[0])
    newmatrix = [[] for _ in range((1+1+2)*width)]
    e_raisedto_stuff = do_e_raisedto_stuff(matrix2D)
    x_squared = do_x_cubed(matrix2D)
    x_cubed = do_x_cubed(matrix2D)
    for i in range(length):
        for j in range(width):
            newmatrix[i,j] = matrix2D[i,j]
    for i in range(length):
        for j in range(width):
            newmatrix[i,j+width] = e_raisedto_stuff[i,j]
    for i in range(length):
        for j in range(width):
            newmatrix[i,j+2*width] = x_squared[i,j]
    for i in range(length):
        for j in range(width):
            newmatrix[i,j+3*width] = x_cubed[i,j]
    return newmatrix

if __name__ == '__main__':
    scaler = Scaler() #use of scaler is optional
    train_features, train_targets = get_features('data/train.csv',True,scaler), get_targets('data/train.csv')
    dev_features, dev_targets = get_features('data/dev.csv',False,scaler), get_targets('data/dev.csv')
    
    size = 25000
    train_features= train_features[0:size]
    train_targets= train_targets[0:size]
    dev_features= dev_features[0:size]
    dev_targets= dev_targets[0:size]

    a_solution = analytical_solution(train_features, train_targets, C=8.0617)
    print('evaluating analytical_solution...')
    dev_loss=do_evaluation(dev_features, dev_targets, a_solution)
    train_loss=do_evaluation(train_features, train_targets, a_solution)
    print('analytical_solution \t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))
    print('analytical_solution \t train_loss squared: {}, dev_loss squared: {} '.format(train_loss*train_loss, dev_loss*dev_loss))
    train_loss_analytical = train_loss
    dev_loss_analytical = dev_loss

    print('training LR using gradient descent...')
    gradient_descent_soln = do_gradient_descent(train_features, train_targets, dev_features, dev_targets,
                        lr=0.00001, C=0.001, batch_size=128, max_steps=150000, eval_steps=10000)
    
    print(gradient_descent_soln)
    print('evaluating iterative_solution...')
    dev_loss=do_evaluation(dev_features, dev_targets, gradient_descent_soln)
    train_loss=do_evaluation(train_features, train_targets, gradient_descent_soln)
    print('gradient_descent_soln \t train loss: {}, dev_loss: {} '.format(train_loss, dev_loss))
    # plot_trainsize_losses()

feature_order = ['bias','latitude',
 'longitude',
 'brightness',
 'scan',
 'track',
 'acq_time',
 'satellite',
 'confidence',
 'bright_t31',
 'daynight',
 'month',
 'year',
 'day']
for i in range(len(feature_order)):
  print(feature_order[i],gradient_descent_soln[i])

test_features= get_features('data/test.csv',False,scaler)
test_predictions = get_predictions(test_features,gradient_descent_soln)

np.savetxt("submission.csv", test_predictions, delimiter=",")

170.36674574729219**2

